# Smart Handover â€“ Automatisierte Umsetzung (ZIP-Output, v3 mit KPI-Policy)

> **Ziel:** Der Code-Chat setzt die Aufgabe **vollstÃ¤ndig und selbstÃ¤ndig** um und liefert ein **ZIP-Paket** mit **fertigen, vollstÃ¤ndigen Dateien**.  
> **Dein Aufwand:** ZIP entpacken â†’ Tests laufen lassen â†’ commit/push.  
> **NEU (Pflicht):** WÃ¤hrend der Umsetzung werden **KPI-Kennzahlen** erfasst (Ampel) und **Quality-Gates** eingehalten. **Jeder** Python-Aufruf auÃŸerhalb von Pytest lÃ¤uft Ã¼ber `tools\run_with_kpi.cmd`.

---

## 0) Projektstruktur (lebendiges Dokument)

- FÃ¼ge hier den **aktuellen Projektbaum** ein (aus Repo).  
- Regeln:
  - Verwende **exakt diese Struktur**.  
  - Neue Dateien in passende Ordner (z. B. `ingestion/`, `data/`, `tests/`).  
  - Importe an diese Struktur anpassen.  
  - Am Ende der Umsetzung: **aktualisierte Projektstruktur** in `AENDERUNGSPROTOKOLL.md` ergÃ¤nzen.

---

## 1) Kontext

- **Projekt:** MT Prompt Engine  
- **Quelle:** `BACKLOG.md` (Story-/Bug-ID), `PROJECT_STATUS.md`  
- **Ziel-Chat:** Code-Chat (Implementer)  
- **Branch:** `feature/<ID>-<kurzname>`

---

## 2) Aufgabe

Setze **<ID â€“ Kurztitel>** **vollstÃ¤ndig** um:

- **Neue Dateien vollstÃ¤ndig erstellen**  
- **GeÃ¤nderte Dateien als komplette Version liefern** (keine Diffs)  
- **Dokumentation aktualisieren**: `CHANGELOG.md`, `PROJECT_STATUS.md`  
- **Tests hinzufÃ¼gen** (pytest, optional UI-Smoketests)  
- **Commit-Message vorbereiten`**

---

## 3) Laufumgebung

- **Python:** 3.13  
- **PowerShell:** 7.x empfohlen (5.1 mÃ¶glich)  
- **pytest:** verwenden (JUnit/HTML-Report)  
- **requirements.txt:** nutzen (keine unnÃ¶tigen Zusatzlibs)

**Setup (PowerShell):**
```powershell
py -3.13 -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
pytest -q
```

---

## 3a) KPI-Policy (Pflicht)

**Ziel der KPIs:**  
- **Pytest-Erfolgsrate** (Fenster, standard 20 LÃ¤ufe)  
- **Errors/Stunde seit letztem Commit** (inkl. externer Tool-Fehler)  
- **Runs-to-Green** (TestlÃ¤ufe bis zum ersten â€žgrÃ¼nâ€œ)

**Werkzeuge/Artefakte:**  
- `tools/kpi_logger.py` (schreibt `reports/kpi_history.csv`)  
- `tools/kpi_report.py` (Ampel, schreibt `reports/kpi_summary.md`)  
- `tools/run_tests.cmd` (fÃ¼hrt `pytest` â†’ KPI-Logger â†’ KPI-Report aus)  
- `tools/kpi_exec.py` & `tools/run_with_kpi.cmd` (zÃ¤hlen **externe** Python-/Tool-Aufrufe als **extra errors**)  
- Reports: `reports/last/junit.xml`, `reports/last/report.html`, `reports/kpi_history.csv`, `reports/kpi_summary.md`, `reports/tool_history.csv`

**Verbindliche Anwendung wÃ¤hrend der Umsetzung:**
1) **Baseline-Run** zu Chat-Beginn:
   ```bat
   toolsun_tests.cmd
   ```
2) **Nach jedem relevanten Implementierungsschritt**:
   ```bat
   toolsun_tests.cmd
   ```
3) **JEDER Python-Aufruf auÃŸerhalb von Pytest** (z. B. CLIs, Hilfsskripte) wird **ausnahmslos** Ã¼ber den Wrapper gestartet:
   ```bat
   toolsun_with_kpi.cmd python -m <modul> [args...]
   ```
   Dadurch werden **Exitâ‰ 0** als **extra error** in den KPIs erfasst (und in `tool_history.csv` protokolliert).

**KPI-Quality-Gates (Ampel, Pflicht fÃ¼r Abgabe):**
- **Pytest-Erfolgsrate** (letzte 20 LÃ¤ufe):  
  ðŸŸ¢ **â‰¥ 98 %** Â· ðŸŸ¡ 95â€“97 % Â· ðŸ”´ < 95 %  
- **Errors/Stunde seit Commit** (mit extra errors):  
  ðŸŸ¢ **â‰¤ 1.0** Â· ðŸŸ¡ > 1.0â€“3.0 Â· ðŸ”´ > 3.0  
- **Runs-to-Green**:  
  ðŸŸ¢ **â‰¤ 2** Â· ðŸŸ¡ 3â€“4 Â· ðŸ”´ â‰¥ 5

**Abgabekriterium (muss erfÃ¼llt sein):**
- Letzter Testlauf **grÃ¼n**  
- Erfolgsrate **â‰¥ 98 %** *(oder: letzte 5 LÃ¤ufe = 100 %)*  
- Runs-to-Green **â‰¤ 2**  
- Errors/Stunde **â‰¤ 1.0**  
- `reports/kpi_summary.md` zeigt **keine rote Ampel**

---

## 4) Importpfade / Top-Level-Pakete

- Top-Level: `config`, `data`, `docs`, `ingestion`, `models`, `scripts`, `services`, `tests`, `themes`, `ui`, `utils`  
- Beispielimporte:
  - `from data.prompt_repository import PromptRepository`
  - `from ingestion.tag_normalizer import normalize_tags`

---

## 5) Konfig-Pfade (Ground Truth)

- Alias/Mapping: `config/tag_aliases.json`  
- Format: `{ "AI": "artificial-intelligence", "ml": "machine-learning" }`  
- Falls Datei fehlt â†’ leerer Alias-Map, keine Exception.

---

## 6) Implementierungs-Vertrag (Signaturen & RÃ¼ckgaben)

*(Je nach Story anpassen â€“ Beispiel Tag-Normalizer)*

```python
def normalize_tag(tag: str, alias_map: dict[str, str] | None = None) -> str: ...
def normalize_tags(tags: list[str] | None, alias_map: dict[str, str] | None = None) -> list[str]: ...
def load_alias_map(path: str = "config/tag_aliases.json") -> dict[str, str]: ...
def normalize_item_tags(item: dict, alias_map: dict[str, str] | None = None) -> dict: ...
```

---

## 7) FeldprioritÃ¤ten (falls Ingest)

- **title:** Item-Wert bevorzugt; wenn leer â†’ Alternative (Pattern/Ãœberschrift).  
- **tags:** Ã¼bernehmen â†’ normalisieren â†’ leere entfernen.  
- **category:** falls leer â†’ `""`.  
- **description/sample_output:** optional; leere nicht erzwingen.

---

## 8) Golden Samples (Soll-Output)

- **GS-1:** Input `["AI","Gen AI"]` â†’ Output `["artificial-intelligence","generative-ai"]`  
- **GS-2:** Input `["AI","ai","Ai"]` â†’ Output `["artificial-intelligence"]`  
- **GS-3:** Input `None` â†’ Output `[]`

---

## 9) Tests

- Pflicht: Golden-Sample-Tests + Mini-Integrationstests (z. B. `PromptRepository`)  
- **CLI-Integrationstest** (empfohlen): CLI via `subprocess` aus Pytest starten, Exit-Code/Stdout prÃ¼fen.  
- Optional: **UI-Smoketest** (PySide6):  
  - `QApplication` + `MainWindow` starten & sofort schlieÃŸen â†’ prÃ¼ft Imports/Wiring

---

## 9a) KPI-Auswertung (Pflicht vor Abgabe)

- Ampel erstellen/prÃ¼fen:
  ```bat
  python tools\kpi_report.py --window 20
  ```
- `reports/kpi_summary.md` und `reports/kpi_history.csv` als Artefakte ins ZIP aufnehmen.  
- Bei **gelb/rot** â†’ Ursachen beheben, erneut laufen lassen (bis Ampel **grÃ¼n**).

---

## 10) Output (Pflicht)

```
<id>_deliverable.zip
â”œâ”€ changes/ (alle betroffenen Dateien als Vollversion)
â”œâ”€ reports/
â”‚  â”œâ”€ last/junit.xml
â”‚  â”œâ”€ last/report.html
â”‚  â”œâ”€ kpi_history.csv
â”‚  â”œâ”€ kpi_summary.md
â”‚  â””â”€ tool_history.csv
â”œâ”€ COMMIT_MESSAGE.txt
â””â”€ AENDERUNGSPROTOKOLL.md  (inkl. Projektstruktur nach Umsetzung)
```

---

## 11) Commit-Message (Format)

```
feat: <ID> <Kurztitel> â€“ <kurzer Nutzen>

- Punkt 1
- Punkt 2

KPI: pass-rate â‰¥98 %, runs-to-green â‰¤2, errors/hour â‰¤1.0 (siehe reports/kpi_summary.md)
```

---

## 12) QualitÃ¤tskriterien (Pflicht)

- [ ] Dateien im richtigen Ordner, Importe lauffÃ¤hig  
- [ ] Golden Samples & Tests **grÃ¼n**  
- [ ] (Optional) UI-Smoketest lÃ¤uft fehlerfrei  
- [ ] `CHANGELOG.md` & `PROJECT_STATUS.md` aktualisiert  
- [ ] `AENDERUNGSPROTOKOLL.md` enthÃ¤lt Projektstruktur nach Umsetzung  
- [ ] **KPI-Gates erfÃ¼llt** (siehe 3a)  
- [ ] Keine manuellen EinfÃ¼gungen nÃ¶tig

---

## 13) Beispiel-Aufrufe

**Tests + KPIs (komplett):**
```bat
toolsun_tests.cmd
python tools\kpi_report.py --window 20
```

**Externe Python-Skripte immer Ã¼ber Wrapper:**
```bat
toolsun_with_kpi.cmd python -m ingestion.article_ingestor --file "C:oller\pfad.txt" --source-title "Titel" --category enhancement --tags "article,pattern"
```

**VollstÃ¤ndiger Handover-Prompt an den Code-Chat:**
> â€žHier ist die Story **<ID <Kurztitel>>** aus `BACKLOG.md`.  
> Bitte setze sie **vollstÃ¤ndig** um und liefere ein **ZIP** gemÃ¤ÃŸ `docs/templates/handover_smart.md`.  
> **Keine Diffs**, alle betroffenen `.py` als **finale Vollversion**, inkl. **Tests**, aktualisierten Markdown-Dateien, `AENDERUNGSPROTOKOLL.md` (inkl. Projektstruktur) + `COMMIT_MESSAGE.txt`.  
> **Wichtig:** Halte die **KPI-Policy** ein (Abschnitt 3a). **Jeden** externen Python-Aufruf Ã¼ber `toolsun_with_kpi.cmd`.  
> Ich mÃ¶chte danach nur **entpacken, testen, committen**.â€œ
